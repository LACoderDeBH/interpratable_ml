{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Properties of Explanations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explanation**: relates the feature values of an instance to it model prediction in a humanly understandable way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Properties of Explanation Methods**\n",
    "- Expressive Power: structure of the explanations the medthos is able to generate (e.g. IF-THEN rules, decision tees, weighted sum, natural lang)\n",
    "- Translucency: how much you have to look under the hood of a model e.g. parameters\n",
    "    - methods relying on intrinsically interpretable models like linera regression (model-specific) are highly transluscent (*can rely on more information to generate explanation*)\n",
    "    - methods relying on manipulating inputs and observing rpedictions have 0 transluscency  (*more portable bc treat ML model as a black box*)  \n",
    "- Portability: range of ML models it can be used on\n",
    "    - surrogate models -> highest portability\n",
    "    - methods that only work for RNNs -> low portability\n",
    "- Algo complexity: computational complexity (consider computation time as a bottleneck in generating explanations_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Properties of Individual Explanations**\n",
    "- Accuracy: how well does explanation predict unseen data? \n",
    "    - High accuracy *Important* if the explanation is used for predictions in place of the MLmodel\n",
    "    - Low accuracy ~ fine if accuracy of the ML model is also low, and if goal is to explain what the black box model does. In this case, only fidelity is important.\n",
    "- Fidelity: How well explanation approximates predeiction of black box model \n",
    "    - accuracy and fidelity closely related i.e. if black box model has Hi Accuracy + Explanation has Hi Fidelity, the Explanation also has Hi Accuracy \n",
    "    - local fidelity: only approx well to model prediction for a subset of data (e.g. local surrogate models) or only an individual data instance (e.g. Shapley Values) \n",
    "- Consistency: How much expalanation differs btwn models trained on same task and that both produce very similar predictions\n",
    "    - e.g. SVM and LR models trained on same task and both produce very similar predictions. Compute expalanation using particular method and analyze how differnet explanations are. If very similar, Highly Consistent\n",
    "    - compares explanations between models\n",
    "    - Hi consistency desirable if models rely on similar relationships. - Stability: Similarity of explanations for similar instances\n",
    "    - compares explanations between similar instances for a fixed model\n",
    "    - always desirable\n",
    "    - High stability: slight variations in the features of an instance do not substantially change the explanation (unless these slight variations also strongly change the prediction). \n",
    "    - Lo stability ~ high variance of the explanation method i.e. explanation method is strongly affected by slight changes of the feature values of the instance to be explained\n",
    "    - Lo stabiloty ~ non-deterministic components of the explanation method, such as a data sampling step, like the local surrogate method uses. \n",
    "- Comprehensibility: How well hum ans understand explanation \n",
    "    - measuring the size of the explanation (e.g. number of features with a non-zero weight in a linear model, number of decision rules, etc) \n",
    "    - testing how well people can predict the behavior of the ML model from the explanations \n",
    "    - comprehensibility of features used in the explanatione.g. complex transformation of features might be less comprehensible than the original features\n",
    "- Certainty: explanation reflect the certainty of the ML model? \n",
    "    - a statement about the models confidence that the prediction is correct. If the model predicts a 4% probability of cancer for one patient, is it as certain as the 4% probability that another patient, with different feature values, received? \n",
    "- Degree of Importance: How well does the explanation reflect the importance of features or parts of the explanation? \n",
    "    - e.g. if a decision rule is generated as an explanation for an individual prediction, *is it clear which of the conditions of the rule was the most important*?\n",
    "- Novelty: Does the explanation reflect whether a data instance to be explained comes from a \"new\" region far removed from the distribution of training data? \n",
    "    - model may be inaccurate and the explanation may be useless\n",
    "    - novelty is related to certainty: *higher the novelty* -> more likely it is that the model will have low certainty due to lack of data\n",
    "- Representativeness: How many instances does an explanation cover? \n",
    "    - Explanations can cover the entire model (e.g. interpretation of weights in a linear regression model) or represent only an individual prediction (e.g. Shapley Values)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.6 HUman-friendly Explanations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
